{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import tweepy \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to display data of each tweet \n",
    "def printtweetdata(n, ith_tweet): \n",
    "    print() \n",
    "    print(f\"Tweet {n}:\") \n",
    "    print(f\"Username:{ith_tweet[0]}\") \n",
    "    print(f\"Description:{ith_tweet[1]}\") \n",
    "    print(f\"Location:{ith_tweet[2]}\") \n",
    "    print(f\"Following Count:{ith_tweet[3]}\") \n",
    "    print(f\"Follower Count:{ith_tweet[4]}\") \n",
    "    print(f\"Total Tweets:{ith_tweet[5]}\") \n",
    "    print(f\"Retweet Count:{ith_tweet[6]}\") \n",
    "    print(f\"Tweet Text:{ith_tweet[7]}\") \n",
    "    print(f\"Hashtags Used:{ith_tweet[8]}\") \n",
    "  \n",
    "  \n",
    "# function to perform data extraction \n",
    "def scrape(words, date_since, numtweet): \n",
    "      \n",
    "    # Creating DataFrame using pandas \n",
    "    db = pd.DataFrame(columns=['username', 'description', 'location', 'following', \n",
    "                               'followers', 'totaltweets', 'retweetcount', 'text', 'hashtags']) \n",
    "      \n",
    "    # We are using .Cursor() to search through twitter for the required tweets. \n",
    "    # The number of tweets can be restricted using .items(number of tweets) \n",
    "    tweets = tweepy.Cursor(api.search, q=words, lang=\"en\", \n",
    "                           since=date_since, tweet_mode='extended').items(numtweet) \n",
    "     \n",
    "    # .Cursor() returns an iterable object. Each item in  \n",
    "    # the iterator has various attributes that you can access to  \n",
    "    # get information about each tweet \n",
    "    list_tweets = [tweet for tweet in tweets] \n",
    "      \n",
    "    # Counter to maintain Tweet Count \n",
    "    i = 1  \n",
    "      \n",
    "    # we will iterate over each tweet in the list for extracting information about each tweet \n",
    "    for tweet in list_tweets: \n",
    "        username = tweet.user.screen_name \n",
    "        description = tweet.user.description \n",
    "        location = tweet.user.location \n",
    "        following = tweet.user.friends_count \n",
    "        followers = tweet.user.followers_count \n",
    "        totaltweets = tweet.user.statuses_count \n",
    "        retweetcount = tweet.retweet_count \n",
    "        hashtags = tweet.entities['hashtags'] \n",
    "          \n",
    "        # Retweets can be distinguished by a retweeted_status attribute, \n",
    "        # in case it is an invalid reference, except block will be executed \n",
    "        try: \n",
    "            text = tweet.retweeted_status.full_text \n",
    "        except AttributeError: \n",
    "            text = tweet.full_text \n",
    "        hashtext = list() \n",
    "        for j in range(0, len(hashtags)): \n",
    "            hashtext.append(hashtags[j]['text']) \n",
    "          \n",
    "        # Here we are appending all the extracted information in the DataFrame \n",
    "        ith_tweet = [username, description, location, following, \n",
    "                     followers, totaltweets, retweetcount, text, hashtext] \n",
    "        db.loc[len(db)] = ith_tweet \n",
    "          \n",
    "        # Function call to print tweet data on screen \n",
    "        printtweetdata(i, ith_tweet) \n",
    "        i = i+1\n",
    "    filename = 'scraped_tweets5.csv'\n",
    "      \n",
    "    # we will save our database as a CSV file. \n",
    "    db.to_csv(filename) \n",
    "  \n",
    "  \n",
    "if __name__ == '__main__': \n",
    "      \n",
    "    # Enter your own credentials obtained  \n",
    "    # from your developer account \n",
    "    consumer_key = \"ry9GMoJRUCdsHo89bkpVosu8T\"\n",
    "    consumer_secret = \"y9Yy5X6RE3n6hEogGhVYCmGDKgDPhWuQB8sexc61xKH01Xv6MA\"\n",
    "    access_key = \"1330861473481052162-NXO8xkGNxf0g6A7wbOjOfMPKZrfUvT\"\n",
    "    access_secret = \"20Ye373RjvjjMfpbPHAW6kbwJURRbqRaQsnMYFBCrZpyw\"\n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret) \n",
    "    auth.set_access_token(access_key, access_secret) \n",
    "    api = tweepy.API(auth,wait_on_rate_limit=True) \n",
    "      \n",
    "    # Enter Hashtag and initial date \n",
    "    print(\"Enter Twitter HashTag to search for\") \n",
    "    words = input() \n",
    "    print(\"Enter Date since The Tweets are required in yyyy-mm--dd\") \n",
    "    date_since = input() \n",
    "      \n",
    "    # number of tweets you want to extract in one run \n",
    "    numtweet = 2\n",
    "    scrape(words, date_since, numtweet) \n",
    "    print('Scraping has completed!') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 10)\n",
      "(3000, 10)\n",
      "(911, 10)\n",
      "(1500, 10)\n",
      "(3000, 10)\n",
      "(13396, 10)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "df_6=pd.read_csv('scraped_tweets6.csv')\n",
    "df_4=pd.read_csv('scraped_tweets4.csv')\n",
    "df_3=pd.read_csv('scraped_tweets3.csv')\n",
    "df_2=pd.read_csv('scraped_tweets2.csv')\n",
    "df_1= pd.read_csv('scraped_tweets1.csv')\n",
    "df=pd.read_csv('scraped_tweets.csv')\n",
    "\n",
    "df_final=pd.concat([df_6,df_4,df_3,df_2,df_1, df], ignore_index=True)\n",
    "print(df_1.shape)\n",
    "print(df_2.shape)\n",
    "print(df_3.shape)\n",
    "print(df_4.shape)\n",
    "print(df_6.shape)\n",
    "print(df_final.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When Diane looks for closure on a treasured re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ðŸ–¤2020.12.15 igs updateðŸ–¤\\nOur baby just signed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@Fox_Claire Well said I bet they love you ðŸ˜‚</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @SSBazinet\\nâœ¨TRACES OF HOMEâœ¨\\nâ€œAt its core,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@DemiSutra Love God</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  When Diane looks for closure on a treasured re...\n",
       "1  ðŸ–¤2020.12.15 igs updateðŸ–¤\\nOur baby just signed ...\n",
       "2        @Fox_Claire Well said I bet they love you ðŸ˜‚\n",
       "3  RT @SSBazinet\\nâœ¨TRACES OF HOMEâœ¨\\nâ€œAt its core,...\n",
       "4                                @DemiSutra Love God"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_drop = ['Unnamed: 0',\n",
    "           'username',\n",
    "           'description',\n",
    "           'location',\n",
    "           'following',\n",
    "           'followers',\n",
    "           'totaltweets',\n",
    "           'hashtags',\n",
    "           'retweetcount']\n",
    "#Eliminer les colonnes dans la liste de to_drop du dataframe df\n",
    "df_final.drop(to_drop, inplace=True, axis=1)\n",
    "#Afficherle rÃ©sultat\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When Diane looks for closure on a treasured re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ðŸ–¤2020.12.15 igs updateðŸ–¤\\nOur baby just signed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@Fox_Claire Well said I bet they love you ðŸ˜‚</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @SSBazinet\\nâœ¨TRACES OF HOMEâœ¨\\nâ€œAt its core,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@DemiSutra Love God</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13390</th>\n",
       "      <td>Buy Now from Amazon &amp;gt; https://t.co/v02ICJkE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13391</th>\n",
       "      <td>Buy Now from Amazon &amp;gt; https://t.co/HCh3IjLn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13392</th>\n",
       "      <td>Buy Now from Amazon &amp;gt; https://t.co/bxBK3lv1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13393</th>\n",
       "      <td>Plenty of nepotism in football you knob.\\n\\nFu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13395</th>\n",
       "      <td>Sore Today Strong Tomorrow Fitness Quote Sport...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9525 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text\n",
       "0      When Diane looks for closure on a treasured re...\n",
       "1      ðŸ–¤2020.12.15 igs updateðŸ–¤\\nOur baby just signed ...\n",
       "2            @Fox_Claire Well said I bet they love you ðŸ˜‚\n",
       "3      RT @SSBazinet\\nâœ¨TRACES OF HOMEâœ¨\\nâ€œAt its core,...\n",
       "4                                    @DemiSutra Love God\n",
       "...                                                  ...\n",
       "13390  Buy Now from Amazon &gt; https://t.co/v02ICJkE...\n",
       "13391  Buy Now from Amazon &gt; https://t.co/HCh3IjLn...\n",
       "13392  Buy Now from Amazon &gt; https://t.co/bxBK3lv1...\n",
       "13393  Plenty of nepotism in football you knob.\\n\\nFu...\n",
       "13395  Sore Today Strong Tomorrow Fitness Quote Sport...\n",
       "\n",
       "[9525 rows x 1 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.drop_duplicates(keep='last')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13396, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import warnings \n",
    "import re\n",
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "        input_txt = re.sub(i, '', input_txt)\n",
    "        \n",
    "    return input_txt   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['tidy_tweet'] = df_final['text'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tidy_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When Diane looks for closure on a treasured re...</td>\n",
       "      <td>When Diane looks closure treasured relationshi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ðŸ–¤2020.12.15 igs updateðŸ–¤\\nOur baby just signed ...</td>\n",
       "      <td>ðŸ–¤2020.12.15 updateðŸ–¤ baby just signed signature...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@Fox_Claire Well said I bet they love you ðŸ˜‚</td>\n",
       "      <td>@Fox_Claire Well said they love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @SSBazinet\\nâœ¨TRACES OF HOMEâœ¨\\nâ€œAt its core,...</td>\n",
       "      <td>@SSBazinet âœ¨TRACES HOMEâœ¨ core, really story ab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@DemiSutra Love God</td>\n",
       "      <td>@DemiSutra Love</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  When Diane looks for closure on a treasured re...   \n",
       "1  ðŸ–¤2020.12.15 igs updateðŸ–¤\\nOur baby just signed ...   \n",
       "2        @Fox_Claire Well said I bet they love you ðŸ˜‚   \n",
       "3  RT @SSBazinet\\nâœ¨TRACES OF HOMEâœ¨\\nâ€œAt its core,...   \n",
       "4                                @DemiSutra Love God   \n",
       "\n",
       "                                          tidy_tweet  \n",
       "0  When Diane looks closure treasured relationshi...  \n",
       "1  ðŸ–¤2020.12.15 updateðŸ–¤ baby just signed signature...  \n",
       "2                    @Fox_Claire Well said they love  \n",
       "3  @SSBazinet âœ¨TRACES HOMEâœ¨ core, really story ab...  \n",
       "4                                    @DemiSutra Love  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['tidy_tweet'] = np.vectorize(remove_pattern)(df_final['text'], \"@[\\w]*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tidy_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When Diane looks for closure on a treasured re...</td>\n",
       "      <td>When Diane looks for closure on a treasured re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ðŸ–¤2020.12.15 igs updateðŸ–¤\\nOur baby just signed ...</td>\n",
       "      <td>ðŸ–¤2020.12.15 igs updateðŸ–¤\\nOur baby just signed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@Fox_Claire Well said I bet they love you ðŸ˜‚</td>\n",
       "      <td>Well said I bet they love you ðŸ˜‚</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @SSBazinet\\nâœ¨TRACES OF HOMEâœ¨\\nâ€œAt its core,...</td>\n",
       "      <td>RT \\nâœ¨TRACES OF HOMEâœ¨\\nâ€œAt its core, it is rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@DemiSutra Love God</td>\n",
       "      <td>Love God</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  When Diane looks for closure on a treasured re...   \n",
       "1  ðŸ–¤2020.12.15 igs updateðŸ–¤\\nOur baby just signed ...   \n",
       "2        @Fox_Claire Well said I bet they love you ðŸ˜‚   \n",
       "3  RT @SSBazinet\\nâœ¨TRACES OF HOMEâœ¨\\nâ€œAt its core,...   \n",
       "4                                @DemiSutra Love God   \n",
       "\n",
       "                                          tidy_tweet  \n",
       "0  When Diane looks for closure on a treasured re...  \n",
       "1  ðŸ–¤2020.12.15 igs updateðŸ–¤\\nOur baby just signed ...  \n",
       "2                    Well said I bet they love you ðŸ˜‚  \n",
       "3  RT \\nâœ¨TRACES OF HOMEâœ¨\\nâ€œAt its core, it is rea...  \n",
       "4                                           Love God  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['tidy_tweet']= df_final['tidy_tweet'].str.lower()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tidy_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When Diane looks for closure on a treasured re...</td>\n",
       "      <td>when diane looks for closure on a treasured re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ðŸ–¤2020.12.15 igs updateðŸ–¤\\nOur baby just signed ...</td>\n",
       "      <td>ðŸ–¤2020.12.15 igs updateðŸ–¤\\nour baby just signed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@Fox_Claire Well said I bet they love you ðŸ˜‚</td>\n",
       "      <td>well said i bet they love you ðŸ˜‚</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @SSBazinet\\nâœ¨TRACES OF HOMEâœ¨\\nâ€œAt its core,...</td>\n",
       "      <td>rt \\nâœ¨traces of homeâœ¨\\nâ€œat its core, it is rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@DemiSutra Love God</td>\n",
       "      <td>love god</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  When Diane looks for closure on a treasured re...   \n",
       "1  ðŸ–¤2020.12.15 igs updateðŸ–¤\\nOur baby just signed ...   \n",
       "2        @Fox_Claire Well said I bet they love you ðŸ˜‚   \n",
       "3  RT @SSBazinet\\nâœ¨TRACES OF HOMEâœ¨\\nâ€œAt its core,...   \n",
       "4                                @DemiSutra Love God   \n",
       "\n",
       "                                          tidy_tweet  \n",
       "0  when diane looks for closure on a treasured re...  \n",
       "1  ðŸ–¤2020.12.15 igs updateðŸ–¤\\nour baby just signed ...  \n",
       "2                    well said i bet they love you ðŸ˜‚  \n",
       "3  rt \\nâœ¨traces of homeâœ¨\\nâ€œat its core, it is rea...  \n",
       "4                                           love god  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"# flags (iOS)\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  # dingbats\n",
    "                               u\"\\u3030\"\n",
    "                           \"]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['tidy_tweet'] = np.vectorize(remove_pattern)(df_final['text'], \"@[\\w]*\")\n",
    "df_final['tidy_tweet'] = df_final['text'].str.replace(emoji_pattern, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tidy_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When Diane looks for closure on a treasured re...</td>\n",
       "      <td>When Diane looks for closure on a treasured re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ðŸ–¤2020.12.15 igs updateðŸ–¤\\nOur baby just signed ...</td>\n",
       "      <td>2020.12.15 igs update\\nOur baby just signed hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@Fox_Claire Well said I bet they love you ðŸ˜‚</td>\n",
       "      <td>@Fox_Claire Well said I bet they love you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @SSBazinet\\nâœ¨TRACES OF HOMEâœ¨\\nâ€œAt its core,...</td>\n",
       "      <td>RT @SSBazinet\\nTRACES OF HOME\\nâ€œAt its core, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@DemiSutra Love God</td>\n",
       "      <td>@DemiSutra Love God</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  When Diane looks for closure on a treasured re...   \n",
       "1  ðŸ–¤2020.12.15 igs updateðŸ–¤\\nOur baby just signed ...   \n",
       "2        @Fox_Claire Well said I bet they love you ðŸ˜‚   \n",
       "3  RT @SSBazinet\\nâœ¨TRACES OF HOMEâœ¨\\nâ€œAt its core,...   \n",
       "4                                @DemiSutra Love God   \n",
       "\n",
       "                                          tidy_tweet  \n",
       "0  When Diane looks for closure on a treasured re...  \n",
       "1  2020.12.15 igs update\\nOur baby just signed hi...  \n",
       "2         @Fox_Claire Well said I bet they love you   \n",
       "3  RT @SSBazinet\\nTRACES OF HOME\\nâ€œAt its core, i...  \n",
       "4                                @DemiSutra Love God  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'theirs', 'mightn', 'themselves', 'because', \"hadn't\", 'were', 'same', \"aren't\", \"doesn't\", 'each', 'have', \"she's\", 'own', 'now', 'them', 'out', 'how', 'herself', 'so', 'just', 'up', 'don', 'aren', 'these', 've', \"wasn't\", 'this', 'who', 'here', 'such', \"shan't\", 'does', \"that'll\", 'her', 'when', 'hasn', 'where', \"mustn't\", 'why', \"weren't\", 'myself', 't', 'yours', 'haven', 'it', 'ours', 'him', 'to', \"needn't\", 's', 'but', 'are', 'shan', 'if', \"shouldn't\", 'very', 'itself', 'nor', 'between', 'in', 'until', 'whom', 'further', 'hadn', 'had', 'with', 'only', 'being', 'as', 'doing', \"you'll\", \"couldn't\", 'i', 'not', 'will', 'me', \"mightn't\", 'is', 'd', 'those', 'before', 'you', 'all', 'few', 'can', 'on', 'doesn', 'been', 'than', 'about', 'off', 'ain', 'y', 'under', 'most', \"wouldn't\", \"should've\", 'or', \"it's\", 'shouldn', 'my', 'during', 'more', 'we', 'other', 'do', 'wouldn', 'the', 'at', 'through', \"haven't\", 'there', 'won', 'needn', 'against', 'having', 'm', 'his', 'didn', 'while', 'she', 'by', 'down', 'they', \"isn't\", 'ma', 'too', 'its', 'couldn', 'and', 'mustn', 'from', 'below', 'll', \"won't\", 'a', 'yourselves', 'of', 'above', \"you're\", \"don't\", 'which', 'ourselves', 'weren', 'your', 'should', 'our', 'both', 'no', 'isn', 'wasn', 'an', \"you'd\", 'their', 'once', \"hasn't\", 'any', 'for', \"you've\", 'was', 'am', 'o', 'yourself', 'himself', 'that', 'has', 'over', 'be', 'did', 'some', 'into', 'hers', 'after', 'then', 'he', \"didn't\", 'again', 'what', 're'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words=set(stopwords.words(\"english\"))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenized_tweet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-0cc5e3c4d3e5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenized_tweet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokenized_tweet\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m             \u001b[0mtokenized_tweet\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenized_tweet\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenized_tweet' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenized_tweet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-bab2c52a91dd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mstemmer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtokenized_tweet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenized_tweet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# stemming\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mtokenized_tweet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenized_tweet' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "tokenized_tweet = tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x]) # stemming\n",
    "tokenized_tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(tokenized_tweet)):\n",
    "    tokenized_tweet[i] = ' '.join(tokenized_tweet[i])\n",
    "\n",
    "df['tidy_tweet'] = tokenized_tweet\n",
    "tokenized_tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "all_words = ' '.join([text for text in df_final['tidy_tweet']])\n",
    "from wordcloud import WordCloud\n",
    "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)\n",
    "\n",
    "plt.figure(figsize=(10, 15))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install wordcloud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "ax = sns.barplot(data=d, x= \"Hashtag\", y = \"Count\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'tidy_tweet'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2645\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2646\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2647\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'tidy_tweet'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-9e57f9d0df74>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtfidf_vectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.90\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# TF-IDF feature matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtfidf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfidf_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_final\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tidy_tweet'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2798\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2799\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2800\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2801\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2802\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2646\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2647\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2648\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2649\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2650\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'tidy_tweet'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n",
    "# TF-IDF feature matrix\n",
    "tfidf = tfidf_vectorizer.fit_transform(df_final['tidy_tweet'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfidf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-de5e56cb09c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'tfidf' is not defined"
     ]
    }
   ],
   "source": [
    "tfidf.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
